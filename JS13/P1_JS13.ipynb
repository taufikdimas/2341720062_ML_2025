{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMD2pZroNsUnDcOS+BqO6xa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Praktikum 1\n","Praktikum 1 ini akan membuat JST sederhana (2 layer) dengan forward pass dan backpropagation manual.\n","\n","Backpropagation adalah sebuah algoritma untuk melatih jaringan saraf tiruan dengan cara mengoreksi kesalahan. Algoritma ini bekerja dengan cara menghitung selisih antara keluaran yang dihasilkan jaringan dan keluaran yang seharusnya (kesalahan), lalu memperbarui bobot dan bias jaringan secara berulang dari keluaran ke masukan untuk meminimalkan kesalahan tersebut. Cara kerja backpropagation\n","\n","- Perambatan maju (forward pass): Data masukan diproses melalui jaringan dari lapisan masukan ke lapisan keluaran untuk menghasilkan prediksi awal.\n","\n","- Hitung kesalahan: Selisih antara keluaran prediksi dan keluaran target dihitung menggunakan fungsi kerugian.\n","\n","- Perambatan mundur (backward pass): Kesalahan disebarkan kembali ke belakang melalui jaringan dari lapisan keluaran ke lapisan masukan untuk menghitung gradien atau turunan parsial dari fungsi kerugian terhadap bobot dan bias.\n","\n","- Perbarui bobot: Bobot dan bias disesuaikan menggunakan algoritma penurunan gradien untuk mengurangi kesalahan pada iterasi berikutnya. <br>\n","\n","Langkah:\n","1. Buat dataset sederhana (XOR).\n","2. Inisialisasi bobot dan bias.\n","3. Implementasikan forward pass.\n","4. Hitung error dan lakukan backpropagation.\n","5. Update bobot menggunakan gradient descent."],"metadata":{"id":"QCxqRkx3rgtR"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWnsbQjdkuLA","executionInfo":{"status":"ok","timestamp":1763815933620,"user_tz":-420,"elapsed":531,"user":{"displayName":"Taufik Dimas Edystara","userId":"10496207114828211202"}},"outputId":"84dffc89-2c3d-4a4c-b45a-d2e1bba97ad4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.2708653390982532\n","Epoch 1000, Loss: 0.24700221798634414\n","Epoch 2000, Loss: 0.22163637175728887\n","Epoch 3000, Loss: 0.16733408856600135\n","Epoch 4000, Loss: 0.14360069722552177\n","Epoch 5000, Loss: 0.13564503632839234\n","Epoch 6000, Loss: 0.13220685240769198\n","Epoch 7000, Loss: 0.1303724998450319\n","Epoch 8000, Loss: 0.1292525928057126\n","Epoch 9000, Loss: 0.12850473083219477\n","Prediksi:\n","[[0.05438475]\n"," [0.95680334]\n"," [0.49469552]\n"," [0.50173377]]\n"]}],"source":["import numpy as np\n","\n","# Dataset XOR\n","X = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y = np.array([[0],[1],[1],[0]])\n","\n","# Parameter\n","input_size = 2\n","hidden_size = 2\n","output_size = 1\n","lr = 0.1\n","\n","# Inisialisasi bobot\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","# Fungsi aktivasi\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Training\n","for epoch in range(10000):\n","    # Forward pass\n","    z1 = np.dot(X, W1) + b1\n","    a1 = sigmoid(z1)\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)\n","\n","    # Hitung error\n","    error = y - a2\n","\n","    # Backpropagation\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    # Update bobot\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","    if epoch % 1000 == 0:\n","        loss = np.mean(np.square(error))\n","        print(f\"Epoch {epoch}, Loss: {loss}\")\n","\n","# Output akhir\n","print(\"Prediksi:\")\n","print(a2)"]},{"cell_type":"markdown","source":["# Tugas\n"],"metadata":{"id":"BgRwGyQMti1s"}},{"cell_type":"markdown","source":["## 1. Ubah jumlah neuron hidden layer menjadi 3."],"metadata":{"id":"vqSmA6WwtlG1"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Dataset XOR\n","X = np.array([[0,0],\n","              [0,1],\n","              [1,0],\n","              [1,1]])\n","y = np.array([[0],\n","              [1],\n","              [1],\n","              [0]])\n","\n","# Parameter\n","input_size = 2\n","hidden_size = 3\n","output_size = 1\n","lr = 0.1\n","np.random.seed(42)\n","\n","# Inisialisasi bobot\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","# Fungsi aktivasi\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","# Training\n","for epoch in range(10000):\n","\n","    z1 = np.dot(X, W1) + b1\n","    a1 = sigmoid(z1)\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)\n","\n","    error = y - a2\n","\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","    if epoch % 1000 == 0:\n","        loss = np.mean(np.square(error))\n","        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n","\n","# Output\n","print(\"\\nPrediksi akhir (hidden_size = 3, sigmoid):\")\n","print(a2)\n","final_loss = np.mean(np.square(y - a2))\n","print(\"Final Loss:\", final_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKnIyZZutkwi","executionInfo":{"status":"ok","timestamp":1763815934188,"user_tz":-420,"elapsed":538,"user":{"displayName":"Taufik Dimas Edystara","userId":"10496207114828211202"}},"outputId":"7b9c6f57-9c5b-452d-dd09-506a90b7d139"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.318245\n","Epoch 1000, Loss: 0.205697\n","Epoch 2000, Loss: 0.141854\n","Epoch 3000, Loss: 0.058651\n","Epoch 4000, Loss: 0.020112\n","Epoch 5000, Loss: 0.009992\n","Epoch 6000, Loss: 0.006270\n","Epoch 7000, Loss: 0.004461\n","Epoch 8000, Loss: 0.003421\n","Epoch 9000, Loss: 0.002756\n","\n","Prediksi akhir (hidden_size = 3, sigmoid):\n","[[0.02515564]\n"," [0.95263635]\n"," [0.95122343]\n"," [0.0627247 ]]\n","Final Loss: 0.0022974157268842183\n"]}]},{"cell_type":"markdown","source":["## 2. Bandingkan hasil loss dengan konfigurasi awal."],"metadata":{"id":"dMs48zHQuTRy"}},{"cell_type":"markdown","source":["> Pada arsitektur dengan 2 neuron hidden, jaringan masih kesulitan memisahkan semua pola XOR sehingga final loss berhenti sekitar 0,128 dan beberapa output mendekati 0,5. Setelah jumlah neuron hidden ditambah menjadi 3, jaringan memiliki kapasitas representasi yang lebih besar sehingga mampu mempelajari fungsi XOR dengan lebih baik, ditunjukkan oleh penurunan final loss hingga sekitar 0,0023 dan output yang mendekati nilai target (0 atau 1) pada seluruh kombinasi input"],"metadata":{"id":"t_LNuCScAmNb"}},{"cell_type":"markdown","source":["3.\n","Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."],"metadata":{"id":"q7-aITJ5klTd"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Dataset XOR\n","X = np.array([[0,0],[0,1],[1,0],[1,1]])\n","y = np.array([[0],[1],[1],[0]])\n","\n","# Parameter\n","input_size = 2\n","hidden_size = 2\n","output_size = 1\n","lr = 0.1\n","epochs = 10000\n","\n","# Fungsi aktivasi\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    # x di sini adalah output sigmoid (a)\n","    return x * (1 - x)\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    # x di sini adalah z (pre-activation)\n","    return (x > 0).astype(float)\n","\n","# ==============================\n","# 1) MODEL DENGAN HIDDEN SIGMOID\n","# ==============================\n","np.random.seed(42)  # supaya adil, inisialisasi sama\n","\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","print(\"=== TRAINING MODEL HIDDEN SIGMOID ===\")\n","for epoch in range(epochs):\n","    # Forward pass\n","    z1 = np.dot(X, W1) + b1\n","    a1 = sigmoid(z1)          # hidden pakai sigmoid\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)          # output pakai sigmoid\n","\n","    # Hitung error\n","    error = y - a2\n","    loss = np.mean(np.square(error))\n","\n","    # Print loss tiap 1000 epoch\n","    if epoch % 1000 == 0:\n","        print(f\"Epoch {epoch}, Loss (sigmoid hidden): {loss:.6f}\")\n","\n","    # Backprop\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    # Update bobot\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","# Simpan hasil sigmoid\n","pred_sigmoid = a2\n","loss_sigmoid = np.mean(np.square(y - pred_sigmoid))\n","\n","print(\"\\n=== HASIL AKHIR MODEL HIDDEN SIGMOID ===\")\n","print(\"Prediksi (sigmoid hidden):\")\n","print(pred_sigmoid)\n","print(\"Final Loss (sigmoid hidden):\", loss_sigmoid)\n","print(\"\\n\")\n","\n","\n","# ===========================\n","# 2) MODEL DENGAN HIDDEN ReLU\n","# ===========================\n","np.random.seed(42)  # reset seed supaya bobot awal sama\n","\n","W1 = np.random.randn(input_size, hidden_size)\n","b1 = np.zeros((1, hidden_size))\n","W2 = np.random.randn(hidden_size, output_size)\n","b2 = np.zeros((1, output_size))\n","\n","print(\"=== TRAINING MODEL HIDDEN ReLU ===\")\n","for epoch in range(epochs):\n","    # Forward pass\n","    z1 = np.dot(X, W1) + b1\n","    a1 = relu(z1)             # hidden pakai ReLU\n","    z2 = np.dot(a1, W2) + b2\n","    a2 = sigmoid(z2)          # output tetap sigmoid\n","\n","    # Hitung error\n","    error = y - a2\n","    loss = np.mean(np.square(error))\n","\n","    # Print loss tiap 1000 epoch\n","    if epoch % 1000 == 0:\n","        print(f\"Epoch {epoch}, Loss (ReLU hidden): {loss:.6f}\")\n","\n","    # Backprop\n","    d_a2 = error * sigmoid_derivative(a2)\n","    d_W2 = np.dot(a1.T, d_a2)\n","    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n","\n","    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n","    d_W1 = np.dot(X.T, d_a1)\n","    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n","\n","    # Update bobot\n","    W1 += lr * d_W1\n","    b1 += lr * d_b1\n","    W2 += lr * d_W2\n","    b2 += lr * d_b2\n","\n","# Simpan hasil ReLU\n","pred_relu = a2\n","loss_relu = np.mean(np.square(y - pred_relu))\n","\n","print(\"\\n=== HASIL AKHIR MODEL HIDDEN ReLU ===\")\n","print(\"Prediksi (ReLU hidden):\")\n","print(pred_relu)\n","print(\"Final Loss (ReLU hidden):\", loss_relu)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuWLH8Zlkpqm","executionInfo":{"status":"ok","timestamp":1763815935825,"user_tz":-420,"elapsed":1618,"user":{"displayName":"Taufik Dimas Edystara","userId":"10496207114828211202"}},"outputId":"b3df4e00-4fd8-4484-c8aa-46df2851b7b2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["=== TRAINING MODEL HIDDEN SIGMOID ===\n","Epoch 0, Loss (sigmoid hidden): 0.255830\n","Epoch 1000, Loss (sigmoid hidden): 0.249406\n","Epoch 2000, Loss (sigmoid hidden): 0.245445\n","Epoch 3000, Loss (sigmoid hidden): 0.204707\n","Epoch 4000, Loss (sigmoid hidden): 0.153204\n","Epoch 5000, Loss (sigmoid hidden): 0.138691\n","Epoch 6000, Loss (sigmoid hidden): 0.133594\n","Epoch 7000, Loss (sigmoid hidden): 0.131151\n","Epoch 8000, Loss (sigmoid hidden): 0.129749\n","Epoch 9000, Loss (sigmoid hidden): 0.128849\n","\n","=== HASIL AKHIR MODEL HIDDEN SIGMOID ===\n","Prediksi (sigmoid hidden):\n","[[0.05300868]\n"," [0.49554213]\n"," [0.95091319]\n"," [0.50319888]]\n","Final Loss (sigmoid hidden): 0.1282265715241046\n","\n","\n","=== TRAINING MODEL HIDDEN ReLU ===\n","Epoch 0, Loss (ReLU hidden): 0.261648\n","Epoch 1000, Loss (ReLU hidden): 0.126387\n","Epoch 2000, Loss (ReLU hidden): 0.125405\n","Epoch 3000, Loss (ReLU hidden): 0.125227\n","Epoch 4000, Loss (ReLU hidden): 0.125163\n","Epoch 5000, Loss (ReLU hidden): 0.125116\n","Epoch 6000, Loss (ReLU hidden): 0.125096\n","Epoch 7000, Loss (ReLU hidden): 0.125071\n","Epoch 8000, Loss (ReLU hidden): 0.125063\n","Epoch 9000, Loss (ReLU hidden): 0.125052\n","\n","=== HASIL AKHIR MODEL HIDDEN ReLU ===\n","Prediksi (ReLU hidden):\n","[[0.50006507]\n"," [0.98947214]\n"," [0.50006507]\n"," [0.00881948]]\n","Final Loss (ReLU hidden): 0.1250471569001235\n"]}]},{"cell_type":"markdown","source":["> Dengan 2 neuron hidden, baik aktivasi sigmoid maupun ReLU sama-sama belum mampu mempelajari pola XOR secara sempurna (masih ada dua kombinasi input yang diprediksi mendekati 0,5). Namun, model dengan aktivasi ReLU menghasilkan nilai loss sedikit lebih kecil (0,1250) dibanding sigmoid (0,1282), karena pada beberapa input seperti [0,1] dan [1,1] prediksinya sangat dekat dengan nilai target. Hal ini menunjukkan bahwa pemilihan fungsi aktivasi dapat memengaruhi seberapa baik jaringan belajar, meskipun dengan arsitektur kecil seperti ini perbedaannya belum terlalu signifikan."],"metadata":{"id":"s7GLWabUl2e-"}}]}